{"cells":[{"cell_type":"markdown","metadata":{"id":"Cs7pSB06lzfF"},"source":["### 0. 실행전 유의사항"]},{"cell_type":"markdown","metadata":{"id":"vs29UIh-_250"},"source":["코랩 PRO를 통해서 51GB DRAM과 15GB VRAM을 활용할 수 있게 되었다.\n","1. 가장 먼저 GPU가 잘 설정되어 있는지 확인한다.\n","2. 환경변수 CUDA_VISIBLE_DEVICES를 0으로 설정한다.\n","3. 파이썬 버전을 3.10으로 되어있는 지 확인한다.\n","4. 구글 드라이브에 미리 OpenFedLLM을 깃 클론해둔다.\n","5. OpenFedLLM의 requirements.txt에서 install==1.3.5를 미리 삭제해둔다.\n","6. OpenFedLLM에 install-1.3.5-py3-none-any.whl 파일을 넣어둔다.\n","7. OpenFedLLM에 output 폴더를 만들어 둔다.\n","8. cd 명령어를 통해서 OpenFedLLM 폴더 내부로 이동한다\n","9. requirements를 설치한다.\n","10. install-1.3.5 패키지를 수동으로 설치한다.\n","11. 허깅페이스허브에 read 토큰으로 로그인한다. (필수적 사항인지는 모름)\n","12. setup.sh를 실행한다.\n","13. 사양을 덜 먹도록 training_scripts/run_sft.sh를 적절하게 수정해둔다\n","14. training_scripts/run_sft.sh를 실행하면 연합 학습이 시작된다."]},{"cell_type":"markdown","metadata":{"id":"fL3TxLtWl5pO"},"source":["### 1. OpenFed 구현"]},{"cell_type":"markdown","metadata":{"id":"PIM0bA1MivIF"},"source":["런타임 유형변경 => GPU T4(16GB) < L4 (24GB) < A100 (40GB) < TPU (64GB)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3374,"status":"ok","timestamp":1731434871609,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"},"user_tz":-540},"id":"BNgM2-rgKMVZ","outputId":"187bd152-b30d-49ac-a32e-ef0a93e1e75d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":970,"status":"ok","timestamp":1731434883648,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"},"user_tz":-540},"id":"ZktcswzpQXhH","outputId":"c6d65ff7-69c0-4096-feb6-15e749150d02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Nov 12 18:08:00 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n","| N/A   47C    P8              16W /  72W |      1MiB / 23034MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","Python 3.10.12\n"]}],"source":["!nvidia-smi\n","!python --version"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VrS8wz66ay4O","executionInfo":{"status":"ok","timestamp":1731434885200,"user_tz":-540,"elapsed":1004,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"}}},"outputs":[],"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # GPU ID를 0으로 설정"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"C0dTRK_yMNTS","executionInfo":{"status":"ok","timestamp":1731434885749,"user_tz":-540,"elapsed":1,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"}}},"outputs":[],"source":["#!git clone --recursive --shallow-submodules https://github.com/rui-ye/OpenFedLLM.git   # 깃클론 1번만"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":8292,"status":"ok","timestamp":1731434894557,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"},"user_tz":-540},"id":"JO057cgKMSEf","outputId":"49487b2f-31ed-4d8b-f621-d537d2625ed0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate==0.21.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.21.0)\n","Requirement already satisfied: aiohttp==3.8.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.8.5)\n","Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.3.1)\n","Requirement already satisfied: annotated-types==0.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.6.0)\n","Requirement already satisfied: async-timeout==4.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.0.3)\n","Requirement already satisfied: attrs==23.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (23.1.0)\n","Requirement already satisfied: bitsandbytes==0.40.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.40.2)\n","Requirement already satisfied: blessed==1.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.20.0)\n","Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2023.7.22)\n","Requirement already satisfied: charset-normalizer==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.2.0)\n","Requirement already satisfied: cmake==3.27.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.27.2)\n","Requirement already satisfied: contourpy==1.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (1.1.1)\n","Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.12.1)\n","Requirement already satisfied: datasets==2.13.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (2.13.0)\n","Requirement already satisfied: deepspeed==0.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.13.1)\n","Requirement already satisfied: dill==0.3.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.3.6)\n","Requirement already satisfied: docstring-parser==0.15 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (0.15)\n","Requirement already satisfied: filelock==3.12.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (3.12.3)\n","Requirement already satisfied: fire==0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (0.5.0)\n","Requirement already satisfied: fonttools==4.43.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (4.43.1)\n","Requirement already satisfied: frozenlist==1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (1.4.0)\n","Requirement already satisfied: fsspec==2023.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (2023.6.0)\n","Requirement already satisfied: gpustat==1.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (1.1.1)\n","Requirement already satisfied: hjson==3.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (3.1.0)\n","Requirement already satisfied: huggingface-hub==0.16.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (0.16.4)\n","Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (3.4)\n","Requirement already satisfied: Jinja2==3.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (3.1.2)\n","Requirement already satisfied: kiwisolver==1.4.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (1.4.5)\n","Requirement already satisfied: lit==16.0.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 29)) (16.0.6)\n","Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 30)) (3.0.0)\n","Requirement already satisfied: MarkupSafe==2.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 31)) (2.1.3)\n","Requirement already satisfied: matplotlib==3.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (3.8.0)\n","Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 33)) (0.1.2)\n","Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (1.3.0)\n","Requirement already satisfied: multidict==6.0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (6.0.4)\n","Requirement already satisfied: multiprocess==0.70.14 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 36)) (0.70.14)\n","Requirement already satisfied: networkx==3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (3.1)\n","Requirement already satisfied: ninja==1.11.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 38)) (1.11.1.1)\n","Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 39)) (1.25.2)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 40)) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 41)) (11.7.101)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 43)) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 44)) (8.5.0.96)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 45)) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 46)) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 47)) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 48)) (11.7.4.91)\n","Requirement already satisfied: nvidia-ml-py==12.535.108 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 49)) (12.535.108)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 50)) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 51)) (11.7.91)\n","Requirement already satisfied: openai==0.28.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 52)) (0.28.0)\n","Requirement already satisfied: packaging==23.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 53)) (23.1)\n","Requirement already satisfied: pandas==2.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 54)) (2.1.0)\n","Requirement already satisfied: peft==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 55)) (0.4.0)\n","Requirement already satisfied: Pillow==10.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 56)) (10.1.0)\n","Requirement already satisfied: psutil==5.9.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 57)) (5.9.5)\n","Requirement already satisfied: py-cpuinfo==9.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 58)) (9.0.0)\n","Requirement already satisfied: pyarrow==13.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 59)) (13.0.0)\n","Requirement already satisfied: pydantic==2.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 60)) (2.5.3)\n","Requirement already satisfied: pydantic_core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 61)) (2.14.6)\n","Requirement already satisfied: Pygments==2.16.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 62)) (2.16.1)\n","Requirement already satisfied: pynvml==11.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 63)) (11.5.0)\n","Requirement already satisfied: pyparsing==3.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 64)) (3.1.1)\n","Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 65)) (2.8.2)\n","Requirement already satisfied: pytz==2023.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 66)) (2023.3)\n","Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 67)) (6.0.1)\n","Requirement already satisfied: regex==2023.8.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 68)) (2023.8.8)\n","Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 69)) (2.31.0)\n","Requirement already satisfied: rich==13.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 70)) (13.6.0)\n","Requirement already satisfied: safetensors==0.3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 71)) (0.3.3)\n","Requirement already satisfied: scipy==1.11.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 72)) (1.11.2)\n","Requirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 73)) (0.1.99)\n","Requirement already satisfied: shtab==1.6.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 74)) (1.6.4)\n","Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 75)) (1.16.0)\n","Requirement already satisfied: sympy==1.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 76)) (1.12)\n","Requirement already satisfied: tenacity==8.2.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 77)) (8.2.3)\n","Requirement already satisfied: termcolor==2.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 78)) (2.3.0)\n","Requirement already satisfied: tokenizers==0.13.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 79)) (0.13.3)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 80)) (2.0.1)\n","Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 81)) (4.66.1)\n","Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 82)) (4.31.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 83)) (2.0.0)\n","Requirement already satisfied: trl==0.7.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 84)) (0.7.2)\n","Requirement already satisfied: typing_extensions==4.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 85)) (4.7.1)\n","Requirement already satisfied: tyro==0.5.10 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 86)) (0.5.10)\n","Requirement already satisfied: tzdata==2023.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 87)) (2023.3)\n","Requirement already satisfied: urllib3==2.0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 88)) (2.0.4)\n","Requirement already satisfied: wcwidth==0.2.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 89)) (0.2.6)\n","Requirement already satisfied: xxhash==3.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 90)) (3.3.0)\n","Requirement already satisfied: yarl==1.9.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 91)) (1.9.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r requirements.txt (line 40)) (75.1.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r requirements.txt (line 40)) (0.44.0)\n","Collecting bitsandbytes-cuda117\n","  Using cached bitsandbytes_cuda117-0.26.0.post2-py3-none-any.whl.metadata (6.3 kB)\n","Using cached bitsandbytes_cuda117-0.26.0.post2-py3-none-any.whl (4.3 MB)\n","Installing collected packages: bitsandbytes-cuda117\n","Successfully installed bitsandbytes-cuda117-0.26.0.post2\n","Looking in links: ./\n","Processing ./install-1.3.5-py3-none-any.whl\n","install is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"]}],"source":["os.chdir(\"/content/drive/MyDrive/Colab Notebooks/OpenFedLLM\")\n","!pip install -r requirements.txt   # install == 1.3.5 삭제하세요\n","!pip install bitsandbytes-cuda117\n","\n","#디스코드에 있는 파일을 OpenFedLLM 폴더에 넣은 후, install=1.3.5를 수동 설치\n","!pip install install-1.3.5-py3-none-any.whl -f ./ --no-index"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"6DyS3DvVPjIs","executionInfo":{"status":"ok","timestamp":1731434898325,"user_tz":-540,"elapsed":467,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"}}},"outputs":[],"source":["#output 폴더 필요\n","#!mkdir -p ./output"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"2ArKZ2KrO1ZD","executionInfo":{"status":"ok","timestamp":1731434899983,"user_tz":-540,"elapsed":624,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"}}},"outputs":[],"source":["# setup.sh 실행\n","!bash setup.sh"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":55932,"status":"ok","timestamp":1731434955914,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"},"user_tz":-540},"id":"yR-Uts3miadJ","outputId":"f3b97b4f-c0fc-46c7-8f6e-bed38c9445c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.40.2)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Collecting transformers\n","  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n","Collecting huggingface_hub\n","  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.4.0)\n","Collecting peft\n","  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.7.2)\n","Collecting trl\n","  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n","Collecting openai\n","  Downloading openai-1.54.4-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (0.13.1)\n","Collecting deepspeed\n","  Downloading deepspeed-0.15.4.tar.gz (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate>=0.26.0\n","  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.0.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.8.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting safetensors>=0.4.1 (from transformers)\n","  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Collecting tokenizers<0.21,>=0.20 (from transformers)\n","  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Collecting datasets>=2.21.0 (from trl)\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.6.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.5.3)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Collecting typing-extensions>=3.7.4.3 (from huggingface_hub)\n","  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed) (3.1.0)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.1.0)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.1)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n","Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from deepspeed) (12.535.108)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.4)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Collecting pyarrow>=15.0.0 (from datasets>=2.21.0->trl)\n","  Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.1.0)\n","Collecting requests (from transformers)\n","  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n","Collecting tqdm>=4.27 (from transformers)\n","  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.3.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.70.14)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.8.5)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->bitsandbytes) (75.1.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->bitsandbytes) (0.44.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->bitsandbytes) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->bitsandbytes) (16.0.6)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.16.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2023.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\n","Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.12.0-py3-none-any.whl (310 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.54.4-py3-none-any.whl (389 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n","Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.15.4-py3-none-any.whl size=1527829 sha256=22120462879cdc66e6375de8dfb5e68f4d6cb4c64e88da53a8eabbbefc819d0f\n","  Stored in directory: /root/.cache/pip/wheels/74/bc/b6/836d7c3e3093e25502fa9248e0be9e943db245f2806ba1cd19\n","Successfully built deepspeed\n","Installing collected packages: typing-extensions, tqdm, safetensors, requests, pyarrow, huggingface_hub, tokenizers, openai, datasets, transformers, accelerate, trl, peft, deepspeed, bitsandbytes\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.7.1\n","    Uninstalling typing_extensions-4.7.1:\n","      Successfully uninstalled typing_extensions-4.7.1\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.66.1\n","    Uninstalling tqdm-4.66.1:\n","      Successfully uninstalled tqdm-4.66.1\n","  Attempting uninstall: safetensors\n","    Found existing installation: safetensors 0.3.3\n","    Uninstalling safetensors-0.3.3:\n","      Successfully uninstalled safetensors-0.3.3\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 13.0.0\n","    Uninstalling pyarrow-13.0.0:\n","      Successfully uninstalled pyarrow-13.0.0\n","  Attempting uninstall: huggingface_hub\n","    Found existing installation: huggingface-hub 0.16.4\n","    Uninstalling huggingface-hub-0.16.4:\n","      Successfully uninstalled huggingface-hub-0.16.4\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.3\n","    Uninstalling tokenizers-0.13.3:\n","      Successfully uninstalled tokenizers-0.13.3\n","  Attempting uninstall: openai\n","    Found existing installation: openai 0.28.0\n","    Uninstalling openai-0.28.0:\n","      Successfully uninstalled openai-0.28.0\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.13.0\n","    Uninstalling datasets-2.13.0:\n","      Successfully uninstalled datasets-2.13.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.31.0\n","    Uninstalling transformers-4.31.0:\n","      Successfully uninstalled transformers-4.31.0\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.21.0\n","    Uninstalling accelerate-0.21.0:\n","      Successfully uninstalled accelerate-0.21.0\n","  Attempting uninstall: trl\n","    Found existing installation: trl 0.7.2\n","    Uninstalling trl-0.7.2:\n","      Successfully uninstalled trl-0.7.2\n","  Attempting uninstall: peft\n","    Found existing installation: peft 0.4.0\n","    Uninstalling peft-0.4.0:\n","      Successfully uninstalled peft-0.4.0\n","  Attempting uninstall: deepspeed\n","    Found existing installation: deepspeed 0.13.1\n","    Uninstalling deepspeed-0.13.1:\n","      Successfully uninstalled deepspeed-0.13.1\n","  Attempting uninstall: bitsandbytes\n","    Found existing installation: bitsandbytes 0.40.2\n","    Uninstalling bitsandbytes-0.40.2:\n","      Successfully uninstalled bitsandbytes-0.40.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 18.0.0 which is incompatible.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.6.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.0 which is incompatible.\n","langchain 0.3.7 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 2.5.3 which is incompatible.\n","langchain-core 0.3.15 requires packaging<25,>=23.2, but you have packaging 23.1 which is incompatible.\n","pylibcudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 18.0.0 which is incompatible.\n","pymc 5.18.0 requires rich>=13.7.1, but you have rich 13.6.0 which is incompatible.\n","pytensor 2.25.5 requires filelock>=3.15, but you have filelock 3.12.3 which is incompatible.\n","sphinx 8.1.3 requires Pygments>=2.17, but you have pygments 2.16.1 which is incompatible.\n","torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.0.1 which is incompatible.\n","torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-1.1.1 bitsandbytes-0.44.1 datasets-3.1.0 deepspeed-0.15.4 huggingface_hub-0.26.2 openai-1.54.4 peft-0.13.2 pyarrow-18.0.0 requests-2.32.3 safetensors-0.4.5 tokenizers-0.20.3 tqdm-4.67.0 transformers-4.46.2 trl-0.12.0 typing-extensions-4.12.2\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n","Collecting torchvision==0.15.2\n","  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.12.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (1.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.32.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (10.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n","Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchvision\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.20.0+cu121\n","    Uninstalling torchvision-0.20.0+cu121:\n","      Successfully uninstalled torchvision-0.20.0+cu121\n","Successfully installed torchvision-0.15.2\n"]}],"source":["# 필수 패키지 최신 버전 설치 및 업그레이드\n","# 24/10/29 : 패키지 업그레이드 필요\n","#transformer, trl 최신버전 -> mistral 에러 해결\n","\n","!pip install --upgrade bitsandbytes transformers huggingface_hub peft trl openai deepspeed 'accelerate>=0.26.0'\n","\n","# 호환되는 torch 및 torchvision 버전 설치\n","!pip install torch==2.0.1 torchvision==0.15.2\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATnD4yqoRT3t","outputId":"b94e50c6-969f-44aa-de37-961461c11102","executionInfo":{"status":"ok","timestamp":1731434971194,"user_tz":-540,"elapsed":15282,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) N\n","Token is valid (permission: fineGrained).\n","The token `ssumday24` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `ssumday24`\n"]}],"source":["#허깅페이스 토큰사용\n","!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124496,"status":"ok","timestamp":1730289918858,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"},"user_tz":-540},"id":"e2FKx-6LJBLr","outputId":"e396a800-07f8-46f9-ce9b-ca96e8515112"},"outputs":[{"output_type":"stream","name":"stdout","text":["training_scripts/run_sft.sh: line 19: ../prometheus-7b-v2.0: Is a directory\n","2024-10-30 12:03:16.933858: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-10-30 12:03:16.953748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-30 12:03:16.975759: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-30 12:03:16.982310: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-30 12:03:16.998889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-30 12:03:18.243438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","[2024-10-30 12:03:20,393] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","ScriptArguments(model_name_or_path='facebook/opt-125m', dataset_name='vicgalle/alpaca-gpt4', log_with='none', learning_rate=5e-05, batch_size=4, seq_length=512, gradient_accumulation_steps=1, load_in_8bit=True, load_in_4bit=False, use_peft=True, trust_remote_code=False, output_dir='./output/alpaca-gpt4_10000_fedavg_c10s2_i10_b4a1_l512_r16a32_20241030120331', peft_lora_r=16, peft_lora_alpha=32, logging_steps=100, use_auth_token=False, num_train_epochs=3, max_steps=10, save_steps=1000, save_total_limit=10, push_to_hub=False, hub_model_id=None, gradient_checkpointing=True, template='alpaca', seed=2023, dpo_beta=0.1, dataset_sample=10000, local_data_dir=None) FedArguments(fed_alg='fedavg', num_rounds=10, num_clients=10, sample_clients=2, split_strategy='iid', prox_mu=0.01, fedopt_tau=0.001, fedopt_eta=0.001, fedopt_beta1=0.9, fedopt_beta2=0.99, save_model_freq=50)\n","Found cached dataset parquet (/root/.cache/huggingface/datasets/vicgalle___parquet/vicgalle--alpaca-gpt4-69061ea5cb3369b0/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/vicgalle___parquet/vicgalle--alpaca-gpt4-69061ea5cb3369b0/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-be09b7f6682932b5.arrow\n","Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/vicgalle___parquet/vicgalle--alpaca-gpt4-69061ea5cb3369b0/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e39a3d640decc9a0.arrow\n",">> ===== After processing, Dataset vicgalle/alpaca-gpt4 has 10000 examples. =====\n","Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/vicgalle___parquet/vicgalle--alpaca-gpt4-69061ea5cb3369b0/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6c5d7d31d8f07294.arrow\n","config.json: 100% 651/651 [00:00<00:00, 4.21MB/s]\n","pytorch_model.bin: 100% 251M/251M [00:01<00:00, 196MB/s]\n","/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","model.safetensors:   8% 21.0M/251M [00:00<00:05, 40.7MB/s]\n","generation_config.json: 100% 137/137 [00:00<00:00, 894kB/s]\n","model.safetensors:  17% 41.9M/251M [00:00<00:02, 81.8MB/s]trainable params: 589,824 || all params: 125,829,120 || trainable%: 0.4688\n","model.safetensors:  50% 126M/251M [00:00<00:00, 186MB/s] \n","tokenizer_config.json: 100% 685/685 [00:00<00:00, 4.88MB/s]\n","model.safetensors:  63% 157M/251M [00:01<00:00, 203MB/s]\n","vocab.json: 100% 899k/899k [00:00<00:00, 15.3MB/s]\n","model.safetensors:  75% 189M/251M [00:01<00:00, 219MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 51.0MB/s]\n","model.safetensors: 100% 251M/251M [00:01<00:00, 174MB/s]\n","special_tokens_map.json: 100% 441/441 [00:00<00:00, 3.19MB/s]\n","  0% 0/10 [00:00<?, ?it/s]>> ==================== Round 1 : [6, 9] ====================\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:08,  1.06it/s]\u001b[A\n"," 20% 2/10 [00:01<00:04,  1.80it/s]\u001b[A\n"," 30% 3/10 [00:01<00:02,  2.34it/s]\u001b[A\n"," 40% 4/10 [00:01<00:02,  2.69it/s]\u001b[A\n"," 50% 5/10 [00:02<00:01,  2.92it/s]\u001b[A\n"," 60% 6/10 [00:02<00:01,  3.09it/s]\u001b[A\n"," 70% 7/10 [00:02<00:00,  3.26it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.42it/s]\u001b[A\n"," 90% 9/10 [00:03<00:00,  3.46it/s]\u001b[A\n","                          \n","\u001b[A{'train_runtime': 5.1274, 'train_samples_per_second': 7.801, 'train_steps_per_second': 1.95, 'train_loss': 2.3534709930419924, 'epoch': 1.0}\n","  0% 0/10 [00:06<?, ?it/s]\n","100% 10/10 [00:05<00:00,  1.95it/s]\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.44it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.54it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.54it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.53it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.54it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.60it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.69it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.70it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.75it/s]\u001b[A\n","                          \n","\u001b[A{'train_runtime': 3.0951, 'train_samples_per_second': 12.923, 'train_steps_per_second': 3.231, 'train_loss': 2.563076972961426, 'epoch': 1.0}\n","  0% 0/10 [00:11<?, ?it/s]\n","100% 10/10 [00:03<00:00,  3.23it/s]\n"," 10% 1/10 [00:11<01:43, 11.53s/it]>> ==================== Round 2 : [1, 2] ====================\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.68it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.75it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.72it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.79it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.77it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.69it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.67it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.74it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.68it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.0809, 'train_samples_per_second': 12.983, 'train_steps_per_second': 3.246, 'train_loss': 2.459541893005371, 'epoch': 1.0}\n"," 10% 1/10 [00:16<01:43, 11.53s/it]\n","100% 10/10 [00:03<00:00,  3.25it/s]\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.36it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.60it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.64it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.72it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.77it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.70it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.67it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.65it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.64it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.1055, 'train_samples_per_second': 12.88, 'train_steps_per_second': 3.22, 'train_loss': 2.466098594665527, 'epoch': 1.0}\n"," 10% 1/10 [00:20<01:43, 11.53s/it]\n","100% 10/10 [00:03<00:00,  3.22it/s]\n"," 20% 2/10 [00:21<01:22, 10.32s/it]>> ==================== Round 3 : [0, 1] ====================\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.54it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.52it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.65it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.57it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.60it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.64it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.61it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.65it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.66it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.1805, 'train_samples_per_second': 12.576, 'train_steps_per_second': 3.144, 'train_loss': 2.3816768646240236, 'epoch': 1.0}\n"," 20% 2/10 [00:25<01:22, 10.32s/it]\n","100% 10/10 [00:03<00:00,  3.14it/s]\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.38it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.47it/s]\u001b[A\n"," 30% 3/10 [00:00<00:02,  3.44it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.58it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.55it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.67it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.70it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.63it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.64it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.168, 'train_samples_per_second': 12.626, 'train_steps_per_second': 3.157, 'train_loss': 2.175670623779297, 'epoch': 1.0}\n"," 20% 2/10 [00:30<01:22, 10.32s/it]\n","100% 10/10 [00:03<00:00,  3.16it/s]\n"," 30% 3/10 [00:30<01:10, 10.04s/it]>> ==================== Round 4 : [3, 8] ====================\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.48it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.56it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.53it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.53it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.54it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.52it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.53it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.61it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.64it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.2012, 'train_samples_per_second': 12.495, 'train_steps_per_second': 3.124, 'train_loss': 2.301450729370117, 'epoch': 1.0}\n"," 30% 3/10 [00:35<01:10, 10.04s/it]\n","100% 10/10 [00:03<00:00,  3.12it/s]\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.70it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.67it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.57it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.53it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.55it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.50it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.52it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.56it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.52it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.1996, 'train_samples_per_second': 12.502, 'train_steps_per_second': 3.125, 'train_loss': 2.3467840194702148, 'epoch': 1.0}\n"," 30% 3/10 [00:40<01:10, 10.04s/it]\n","100% 10/10 [00:03<00:00,  3.13it/s]\n"," 40% 4/10 [00:40<00:59,  9.94s/it]>> ==================== Round 5 : [3, 4] ====================\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.23it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.50it/s]\u001b[A\n"," 30% 3/10 [00:00<00:02,  3.49it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.57it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.59it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.56it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.54it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.52it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.53it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.1937, 'train_samples_per_second': 12.525, 'train_steps_per_second': 3.131, 'train_loss': 2.00748233795166, 'epoch': 1.0}\n"," 40% 4/10 [00:45<00:59,  9.94s/it]\n","100% 10/10 [00:03<00:00,  3.13it/s]\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.43it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.52it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.53it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.64it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.64it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.66it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.70it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.67it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.71it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.083, 'train_samples_per_second': 12.974, 'train_steps_per_second': 3.244, 'train_loss': 2.460527801513672, 'epoch': 1.0}\n"," 40% 4/10 [00:50<00:59,  9.94s/it]\n","100% 10/10 [00:03<00:00,  3.24it/s]\n"," 50% 5/10 [00:50<00:49,  9.84s/it]>> ==================== Round 6 : [4, 9] ====================\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.64it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.57it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.65it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.67it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.62it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.58it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.64it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.66it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.69it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.1327, 'train_samples_per_second': 12.769, 'train_steps_per_second': 3.192, 'train_loss': 2.4730598449707033, 'epoch': 1.0}\n"," 50% 5/10 [00:54<00:49,  9.84s/it]\n","100% 10/10 [00:03<00:00,  3.19it/s]\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.73it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.69it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.63it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.71it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.68it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.62it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.64it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.72it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.72it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.066, 'train_samples_per_second': 13.046, 'train_steps_per_second': 3.262, 'train_loss': 2.3024030685424806, 'epoch': 1.0}\n"," 50% 5/10 [00:59<00:49,  9.84s/it]\n","100% 10/10 [00:03<00:00,  3.26it/s]\n"," 60% 6/10 [00:59<00:38,  9.75s/it]>> ==================== Round 7 : [1, 9] ====================\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.58it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.63it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.75it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.65it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.59it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.57it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.59it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.57it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.59it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.1581, 'train_samples_per_second': 12.666, 'train_steps_per_second': 3.166, 'train_loss': 2.250905418395996, 'epoch': 1.0}\n"," 60% 6/10 [01:04<00:38,  9.75s/it]\n","100% 10/10 [00:03<00:00,  3.17it/s]\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.36it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.57it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.54it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.67it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.68it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.64it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.64it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.61it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.70it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.1154, 'train_samples_per_second': 12.84, 'train_steps_per_second': 3.21, 'train_loss': 2.3468677520751955, 'epoch': 1.0}\n"," 60% 6/10 [01:09<00:38,  9.75s/it]\n","100% 10/10 [00:03<00:00,  3.21it/s]\n"," 70% 7/10 [01:09<00:29,  9.72s/it]>> ==================== Round 8 : [2, 5] ====================\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.66it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.57it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.64it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.72it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.77it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.78it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.69it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.67it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.68it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.0752, 'train_samples_per_second': 13.007, 'train_steps_per_second': 3.252, 'train_loss': 2.4940940856933596, 'epoch': 1.0}\n"," 70% 7/10 [01:14<00:29,  9.72s/it]\n","100% 10/10 [00:03<00:00,  3.25it/s]\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.58it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.53it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.60it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.62it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.69it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.63it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.62it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.61it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.64it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.1299, 'train_samples_per_second': 12.78, 'train_steps_per_second': 3.195, 'train_loss': 2.3436254501342773, 'epoch': 1.0}\n"," 70% 7/10 [01:18<00:29,  9.72s/it]\n","100% 10/10 [00:03<00:00,  3.20it/s]\n"," 80% 8/10 [01:18<00:19,  9.66s/it]>> ==================== Round 9 : [3, 5] ====================\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.28it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.49it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.66it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.73it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.67it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.70it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.73it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.76it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.81it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.1901, 'train_samples_per_second': 12.539, 'train_steps_per_second': 3.135, 'train_loss': 2.3303905487060548, 'epoch': 1.0}\n"," 80% 8/10 [01:23<00:19,  9.66s/it]\n","100% 10/10 [00:03<00:00,  3.14it/s]\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.63it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.65it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.60it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.67it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.63it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.59it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.62it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.67it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.65it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.1039, 'train_samples_per_second': 12.887, 'train_steps_per_second': 3.222, 'train_loss': 2.27884521484375, 'epoch': 1.0}\n"," 80% 8/10 [01:28<00:19,  9.66s/it]\n","100% 10/10 [00:03<00:00,  3.22it/s]\n"," 90% 9/10 [01:28<00:09,  9.68s/it]>> ==================== Round 10 : [5, 7] ====================\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.46it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.37it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.51it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.52it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.63it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.65it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.69it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.65it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.64it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.2016, 'train_samples_per_second': 12.494, 'train_steps_per_second': 3.123, 'train_loss': 2.314602279663086, 'epoch': 1.0}\n"," 90% 9/10 [01:33<00:09,  9.68s/it]\n","100% 10/10 [00:03<00:00,  3.12it/s]\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","\n","Map:   0% 0/40 [00:00<?, ? examples/s]\u001b[A\n","                                      \u001b[A/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  super().__init__(\n","max_steps is given, it will override any value given in num_train_epochs\n","\n","  0% 0/10 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","\n"," 10% 1/10 [00:00<00:02,  3.43it/s]\u001b[A\n"," 20% 2/10 [00:00<00:02,  3.55it/s]\u001b[A\n"," 30% 3/10 [00:00<00:01,  3.57it/s]\u001b[A\n"," 40% 4/10 [00:01<00:01,  3.64it/s]\u001b[A\n"," 50% 5/10 [00:01<00:01,  3.64it/s]\u001b[A\n"," 60% 6/10 [00:01<00:01,  3.61it/s]\u001b[A\n"," 70% 7/10 [00:01<00:00,  3.65it/s]\u001b[A\n"," 80% 8/10 [00:02<00:00,  3.63it/s]\u001b[A\n"," 90% 9/10 [00:02<00:00,  3.64it/s]\u001b[A\n","                                  \n","\u001b[A{'train_runtime': 3.126, 'train_samples_per_second': 12.796, 'train_steps_per_second': 3.199, 'train_loss': 2.382647705078125, 'epoch': 1.0}\n"," 90% 9/10 [01:38<00:09,  9.68s/it]\n","100% 10/10 [00:03<00:00,  3.20it/s]\n","100% 10/10 [01:38<00:00,  9.83s/it]\n"]}],"source":["# run_sft.sh 실행\n","\n","!bash training_scripts/run_sft.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1730289528231,"user":{"displayName":"Justin Lee","userId":"09653251488244749350"},"user_tz":-540},"id":"Oy47rWoZ3iou","outputId":"860d00fe-6259-4f84-dcb2-9cc4aeed9cd2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/OpenFedLLM'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}],"source":["import os\n","os.getcwd()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Cs7pSB06lzfF"],"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyME6OCinRKiq0N4goYrWSUJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}